{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb52c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy scikit-learn rapidfuzz unidecode matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from rapidfuzz import process,fuzz\n",
    "import unidecode\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")   # or \"Qt5Agg\" if you have PyQt installed\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18367da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning process\n",
    "\n",
    "#1) chech for missing values and calculating the percentage of missing values per column\n",
    "print(data.isnull().sum())\n",
    "print((data.isnull().sum()/len(data))*100)\n",
    "\n",
    "#data is not having any missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) check for duplicated values\n",
    "data.duplicated().any() # returns True if there are any duplicated rows, False otherwise\n",
    "data.duplicated().sum() # returns the number of duplicated rows\n",
    "data[data.duplicated()] # returns the duplicated rows\n",
    "\n",
    "#data is not having any duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31360b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) Handle inconsistent data entries\n",
    "#check if the columns is in the right data type\n",
    "\n",
    "print(data.dtypes)\n",
    "# we have Date as object we need to convert it to datetime64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c95f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_col=data[['Product Name','Date','Product Category','Region','Payment Method']]\n",
    "for col in object_col:\n",
    "    data[col].str.strip() #this removes the space at the beginning and end of the string\n",
    "data.to_csv('Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e12e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'], dayfirst=True, errors='coerce')\n",
    "#change the date from object to datetime64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561439e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize categorical values\n",
    "\n",
    "data['Date'].value_counts()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Product Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = data['Product Name'].unique()\n",
    "similar_pairs=[]\n",
    "for product in choices:\n",
    "    matches = process.extract(product, choices, limit=None, scorer=fuzz.partial_ratio)\n",
    "    for match, score,index in matches:\n",
    "        if score >=75 and product !=match:\n",
    "            pair=tuple(sorted([product,match]))\n",
    "            if pair not in similar_pairs:\n",
    "                similar_pairs.append((pair[0],pair[1],score)) \n",
    "\n",
    "for p1,p2,score in similar_pairs:\n",
    "    print(f\"'{p1}' and '{p2}' have a similarity score of {score}\") \n",
    "\n",
    "data['Product Name'].replace('MacBook Pro 16-inch','Apple MacBook Pro 16-inch',inplace=True)\n",
    "data['Product Name'].replace('Nike Air Force 1','Nike Air Force 1 Sneakers',inplace=True)\n",
    "data['Product Name'].replace('Adidas Ultraboost Shoes','Adidas Ultraboost Running Shoes',inplace=True)\n",
    "data['Product Name'].replace('Yeti Rambler Tumbler','Yeti Rambler 20 oz Tumbler',inplace=True)\n",
    "data.to_csv('Data.csv', index=False)\n",
    "\n",
    "#the product names have more than 200 unique objects and repeated either once or twice\n",
    "#so it will be hard to check one by one if the names are repeated differently in different way or not\n",
    "#i used rapidfuzz library to find the similar names based on similarity score\n",
    "# after getting the similar names i replaced them with the correct names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72bb6bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Region",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ba5c7018-57cb-4b14-a623-a73222cdc95b",
       "rows": [
        [
         "North America",
         "80"
        ],
        [
         "Europe",
         "80"
        ],
        [
         "Asia",
         "80"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3
       }
      },
      "text/plain": [
       "Region\n",
       "North America    80\n",
       "Europe           80\n",
       "Asia             80\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f1d3408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Payment Method",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "6b877f4f-34ad-402f-ac03-e7e4adb5c859",
       "rows": [
        [
         "Credit Card",
         "120"
        ],
        [
         "PayPal",
         "80"
        ],
        [
         "Debit Card",
         "40"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3
       }
      },
      "text/plain": [
       "Payment Method\n",
       "Credit Card    120\n",
       "PayPal          80\n",
       "Debit Card      40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Payment Method'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "984b6d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for non-ASCII characters in 'Product Name' column\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "mask=data['Product Name'].apply(lambda x: not all(ord(c) <128 for c in str(x))) \n",
    "#this returns a boolean series where True indicates the presence of non-ASCII characters\n",
    "data[mask]\n",
    "\n",
    "#function to normalize text by removing accents and special characters\n",
    "def normalize_text(text): \n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8') \n",
    "data['Product Name'] = data['Product Name'].apply(normalize_text)\n",
    "data.to_csv('Data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for invalid values like -ve values or unreasonable values in the numerical columns\n",
    "invalid_unitSolds=data[data['Units Sold']<0]\n",
    "invalid_unitPrice=data[data['Unit Price']<0]\n",
    "invalid_totalRevenue=data[data['Total Revenue']<0]\n",
    "\n",
    "print(invalid_unitSolds)\n",
    "print(invalid_unitPrice)\n",
    "print(invalid_totalRevenue)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c832f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohamed\\AppData\\Local\\Temp\\ipykernel_9964\\1829644448.py:24: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax1.boxplot(data['Units Sold'],vert=False,labels=['Units Sold'])\n",
      "C:\\Users\\mohamed\\AppData\\Local\\Temp\\ipykernel_9964\\1829644448.py:29: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax2.boxplot(data['Unit Price'],vert=False,labels=['Unit Price'])\n",
      "C:\\Users\\mohamed\\AppData\\Local\\Temp\\ipykernel_9964\\1829644448.py:33: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax3.boxplot(data['Total Revenue'],vert=False,labels=['Total Revenue'])\n"
     ]
    }
   ],
   "source": [
    "#4) Handling outliers\n",
    "\n",
    "numeric_cols = data[['Units Sold', 'Unit Price', 'Total Revenue']] \n",
    "\n",
    "#printing the outliers\n",
    "for col in numeric_cols:\n",
    "    Q1=data[col].quantile(0.25)\n",
    "    Q3=data[col].quantile(0.75)\n",
    "    IQR=Q3-Q1\n",
    "    lower=Q1 - 1.5 * IQR\n",
    "    upper=Q3 + 1.5 * IQR\n",
    "    outliers=data[(data[col]<lower) | (data[col]>upper)]\n",
    "    #the series is undergoing a condition it will check if it's true and retrun a series of boolean\n",
    "    #now outlier is a dataframe that contains all the rows that have outliers in that specific column\n",
    "    # if not outliers.empty:\n",
    "    #     print(f\"\\nColumn '{col}' has outliers:\")\n",
    "    #     print(outliers[[col]])\n",
    "\n",
    "\n",
    "#using the box plot to visualize the outliers\n",
    "fig1,ax1=plt.subplots()\n",
    "fig2,ax2=plt.subplots()\n",
    "fig3,ax3=plt.subplots()\n",
    "ax1.boxplot(data['Units Sold'],vert=False,labels=['Units Sold'])\n",
    "ax1.set_title('Box plot for Units Sold')\n",
    "ax1.set_xlabel('No. of Units Sold')\n",
    "\n",
    "\n",
    "ax2.boxplot(data['Unit Price'],vert=False,labels=['Unit Price'])\n",
    "ax2.set_title('Box plot for Unit Price')\n",
    "ax2.set_xlabel('USD$')\n",
    "\n",
    "ax3.boxplot(data['Total Revenue'],vert=False,labels=['Total Revenue'])\n",
    "ax3.set_title('Box plot for Total Revenue')\n",
    "ax3.set_xlabel('USD$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e32eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the data to log scale to reduce the effect of outliers\n",
    "data['Unit Price']=np.log1p(data['Unit Price'])\n",
    "data['Total Revenue']=np.log1p(data['Total Revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#visualizing the data after log transformation\n",
    "fig2,ax2=plt.subplots()\n",
    "fig3,ax3=plt.subplots()\n",
    "\n",
    "ax2.boxplot(data['Unit Price'],vert=False,labels=['Unit Price'])\n",
    "ax2.set_title('Box plot for Unit Price')\n",
    "ax2.set_xlabel('USD$')\n",
    "\n",
    "ax3.boxplot(data['Total Revenue'],vert=False,labels=['Total Revenue'])\n",
    "ax3.set_title('Box plot for Total Revenue')\n",
    "ax3.set_xlabel('USD$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded=pd.get_dummies(data,columns=['Region','Product Category','Payment Method'])\n",
    "bool_cols = data_encoded.select_dtypes(include='bool').columns\n",
    "data_encoded[bool_cols] = data_encoded[bool_cols].astype(int)\n",
    "\n",
    "#we transformed the boolean columns to int to avoid issues during clustering\n",
    "print(data_encoded.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded=data_encoded.drop(['Product Name','Transaction ID'],axis=1)\n",
    "#we don't need the product name as it's too many unique values and it will not help in clustering\n",
    "#the id will alter the clustering results\n",
    "print(data_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b70823",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_encoded.select_dtypes(include='number')\n",
    "print(X.head())\n",
    "kmeans=KMeans(n_clusters=3,random_state=123)\n",
    "data_encoded['Cluster']=kmeans.fit_predict(X)\n",
    "\n",
    "\n",
    "\n",
    "# Check average feature values per cluster\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=data_encoded['Cluster'], cmap='viridis', s=50)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('K-Means Clusters (2D PCA)')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n",
    "#print(data_encoded.head())\n",
    "data_encoded.to_csv('Cleaned_Data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
